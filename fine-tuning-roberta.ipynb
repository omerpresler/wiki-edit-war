{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8e0063",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/papewalycisse/fine-tuning-roberta#Evaluate-the-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca961e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.2\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from transformers==4.40.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from transformers==4.40.2) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from transformers==4.40.2) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from transformers==4.40.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from transformers==4.40.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from transformers==4.40.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from transformers==4.40.2) (2.32.4)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from transformers==4.40.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from transformers==4.40.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tqdm>=4.27->transformers==4.40.2) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests->transformers==4.40.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests->transformers==4.40.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests->transformers==4.40.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests->transformers==4.40.2) (2025.6.15)\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/9.0 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.4/9.0 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.0/9.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.4/9.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 11.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\n",
      "  Attempting uninstall: tokenizers\n",
      "\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "  Attempting uninstall: transformers\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "    Found existing installation: transformers 4.53.0\n",
      "   ---------------------------------------- 0/2 [tokenizers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "    Uninstalling transformers-4.53.0:\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "      Successfully uninstalled transformers-4.53.0\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   -------------------- ------------------- 1/2 [transformers]\n",
      "   ---------------------------------------- 2/2 [transformers]\n",
      "\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.40.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting keras\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting absl-py (from keras)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from keras) (2.1.2)\n",
      "Collecting rich (from keras)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading h5py-3.14.0-cp311-cp311-win_amd64.whl.metadata (2.7 kB)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.16.0-cp311-cp311-win_amd64.whl.metadata (31 kB)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Using cached ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from keras) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from optree->keras) (4.12.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 6.5 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading h5py-3.14.0-cp311-cp311-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 2.4/2.9 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 11.2 MB/s eta 0:00:00\n",
      "Using cached ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl (209 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp311-cp311-win_amd64.whl (314 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, optree, ml-dtypes, mdurl, h5py, absl-py, markdown-it-py, rich, keras\n",
      "\n",
      "   ---- ----------------------------------- 1/9 [optree]\n",
      "   ---- ----------------------------------- 1/9 [optree]\n",
      "   -------- ------------------------------- 2/9 [ml-dtypes]\n",
      "   ----------------- ---------------------- 4/9 [h5py]\n",
      "   ----------------- ---------------------- 4/9 [h5py]\n",
      "   ----------------- ---------------------- 4/9 [h5py]\n",
      "   ----------------- ---------------------- 4/9 [h5py]\n",
      "   ----------------- ---------------------- 4/9 [h5py]\n",
      "   ----------------- ---------------------- 4/9 [h5py]\n",
      "   ---------------------- ----------------- 5/9 [absl-py]\n",
      "   ---------------------- ----------------- 5/9 [absl-py]\n",
      "   -------------------------- ------------- 6/9 [markdown-it-py]\n",
      "   -------------------------- ------------- 6/9 [markdown-it-py]\n",
      "   -------------------------- ------------- 6/9 [markdown-it-py]\n",
      "   -------------------------- ------------- 6/9 [markdown-it-py]\n",
      "   -------------------------- ------------- 6/9 [markdown-it-py]\n",
      "   ------------------------------- -------- 7/9 [rich]\n",
      "   ------------------------------- -------- 7/9 [rich]\n",
      "   ------------------------------- -------- 7/9 [rich]\n",
      "   ------------------------------- -------- 7/9 [rich]\n",
      "   ------------------------------- -------- 7/9 [rich]\n",
      "   ------------------------------- -------- 7/9 [rich]\n",
      "   ------------------------------- -------- 7/9 [rich]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ----------------------------------- ---- 8/9 [keras]\n",
      "   ---------------------------------------- 9/9 [keras]\n",
      "\n",
      "Successfully installed absl-py-2.3.0 h5py-3.14.0 keras-3.10.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 optree-0.16.0 rich-14.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.73.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (2.1.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl (375.9 MB)\n",
      "Downloading grpcio-1.73.1-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 2.1/4.3 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.2/4.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 8.4 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, markdown, grpcio, google-pasta, gast, astunparse, tensorboard, tensorflow\n",
      "\n",
      "   ----------------------------------------  0/16 [libclang]\n",
      "   ----------------------------------------  0/16 [libclang]\n",
      "   -- -------------------------------------  1/16 [flatbuffers]\n",
      "   ----- ----------------------------------  2/16 [wrapt]\n",
      "   ------- --------------------------------  3/16 [werkzeug]\n",
      "   ------- --------------------------------  3/16 [werkzeug]\n",
      "   ------- --------------------------------  3/16 [werkzeug]\n",
      "   ------- --------------------------------  3/16 [werkzeug]\n",
      "   ------- --------------------------------  3/16 [werkzeug]\n",
      "   ---------- -----------------------------  4/16 [termcolor]\n",
      "   ----------------- ----------------------  7/16 [protobuf]\n",
      "   ----------------- ----------------------  7/16 [protobuf]\n",
      "   ----------------- ----------------------  7/16 [protobuf]\n",
      "   ----------------- ----------------------  7/16 [protobuf]\n",
      "   ----------------- ----------------------  7/16 [protobuf]\n",
      "   ----------------- ----------------------  7/16 [protobuf]\n",
      "   -------------------- -------------------  8/16 [opt-einsum]\n",
      "   -------------------- -------------------  8/16 [opt-einsum]\n",
      "   -------------------- -------------------  8/16 [opt-einsum]\n",
      "   ---------------------- -----------------  9/16 [markdown]\n",
      "   ---------------------- -----------------  9/16 [markdown]\n",
      "   ---------------------- -----------------  9/16 [markdown]\n",
      "   ---------------------- -----------------  9/16 [markdown]\n",
      "   ------------------------- -------------- 10/16 [grpcio]\n",
      "   ------------------------- -------------- 10/16 [grpcio]\n",
      "   ------------------------- -------------- 10/16 [grpcio]\n",
      "   ------------------------- -------------- 10/16 [grpcio]\n",
      "   ------------------------- -------------- 10/16 [grpcio]\n",
      "   --------------------------- ------------ 11/16 [google-pasta]\n",
      "   --------------------------- ------------ 11/16 [google-pasta]\n",
      "   ------------------------------ --------- 12/16 [gast]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [tensorboard]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ---------------------------------------- 16/16 [tensorflow]\n",
      "\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 libclang-18.1.1 markdown-3.8.2 opt-einsum-3.4.0 protobuf-5.29.5 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from seaborn) (2.1.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from seaborn) (2.3.0)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading fonttools-4.58.4-cp311-cp311-win_amd64.whl.metadata (108 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\desktop\\university\\datamining\\wiki-project\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading matplotlib-3.10.3-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/8.1 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.9/8.1 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 10.9 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.2-cp311-cp311-win_amd64.whl (222 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.0/2.2 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "\n",
      "   ---------------------------------------- 0/7 [pyparsing]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ---------------------- ----------------- 4/7 [contourpy]\n",
      "   ---------------------- ----------------- 4/7 [contourpy]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------------- 7/7 [seaborn]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 kiwisolver-1.4.8 matplotlib-3.10.3 pyparsing-3.2.3 seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  --upgrade \"transformers==4.40.2\"\n",
    "%pip install datasets\n",
    "%pip install keras\n",
    "%pip install tensorflow\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5027ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece0c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a06835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175au9w</td>\n",
       "      <td>k4efycz</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Israel pulled out of Gaza in 2005 so the 2010 ...</td>\n",
       "      <td>2023-10-11 11:56:11</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175au9w</td>\n",
       "      <td>k4eg0gi</td>\n",
       "      <td>with palestine</td>\n",
       "      <td>The 1946 map should be completely red as the w...</td>\n",
       "      <td>2023-10-11 11:56:49</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175au9w</td>\n",
       "      <td>k4ees0u</td>\n",
       "      <td>with palestine</td>\n",
       "      <td>To be fair. You should count 1947 as the first...</td>\n",
       "      <td>2023-10-11 11:43:13</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175au9w</td>\n",
       "      <td>k4eke8z</td>\n",
       "      <td>with israel</td>\n",
       "      <td>No. Each map is depicting something different....</td>\n",
       "      <td>2023-10-11 12:41:04</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175au9w</td>\n",
       "      <td>k4eipaq</td>\n",
       "      <td>with israel</td>\n",
       "      <td>A history of Israel over the last 100 years, i...</td>\n",
       "      <td>2023-10-11 12:24:44</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id       id           label  \\\n",
       "0  175au9w  k4efycz         neutral   \n",
       "1  175au9w  k4eg0gi  with palestine   \n",
       "2  175au9w  k4ees0u  with palestine   \n",
       "3  175au9w  k4eke8z     with israel   \n",
       "4  175au9w  k4eipaq     with israel   \n",
       "\n",
       "                                                text            timestamp  \\\n",
       "0  Israel pulled out of Gaza in 2005 so the 2010 ...  2023-10-11 11:56:11   \n",
       "1  The 1946 map should be completely red as the w...  2023-10-11 11:56:49   \n",
       "2  To be fair. You should count 1947 as the first...  2023-10-11 11:43:13   \n",
       "3  No. Each map is depicting something different....  2023-10-11 12:41:04   \n",
       "4  A history of Israel over the last 100 years, i...  2023-10-11 12:24:44   \n",
       "\n",
       "    ups  \n",
       "0  1411  \n",
       "1   833  \n",
       "2   457  \n",
       "3   217  \n",
       "4   168  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_data=pd.read_csv(\"data/reddit_comments_clean.csv\")\n",
    "comments_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb5e2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>preview</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>175au9w</td>\n",
       "      <td>682</td>\n",
       "      <td>https://external-preview.redd.it/35qaia1zr0YPI...</td>\n",
       "      <td>r/geopolitics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-11 11:31:27</td>\n",
       "      <td>Is this Palestine-Israel map history accurate?</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1060</td>\n",
       "      <td>https://solidarnost.su/wp-content/uploads/2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1c7lqqt</td>\n",
       "      <td>1375</td>\n",
       "      <td>https://external-preview.redd.it/mKqHVAGaLWiUI...</td>\n",
       "      <td>r/news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-04-19 03:26:18</td>\n",
       "      <td>Israel missiles strike Iran - US officials inf...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>11643</td>\n",
       "      <td>https://www.reuters.com/world/middle-east/isra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1c3pjmi</td>\n",
       "      <td>1492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>r/NoStupidQuestions</td>\n",
       "      <td>Aren't they Jewish? Don't they specifically NO...</td>\n",
       "      <td>2024-04-14 10:26:07</td>\n",
       "      <td>Why do Christian nationalists support Israel, ...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>6048</td>\n",
       "      <td>https://www.reddit.com/r/NoStupidQuestions/com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>175v330</td>\n",
       "      <td>456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>r/geopolitics</td>\n",
       "      <td>Basically the question above. I understand the...</td>\n",
       "      <td>2023-10-12 02:44:41</td>\n",
       "      <td>Why is Israel so significant for the West ?</td>\n",
       "      <td>0.87</td>\n",
       "      <td>366</td>\n",
       "      <td>https://www.reddit.com/r/geopolitics/comments/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>palestisrael</td>\n",
       "      <td>1c3aiv5</td>\n",
       "      <td>5143</td>\n",
       "      <td>https://external-preview.redd.it/8bwtWtE4wx8uq...</td>\n",
       "      <td>r/worldnews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-04-13 19:53:25</td>\n",
       "      <td>Iran launched dozens of drones toward Israel -...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>28884</td>\n",
       "      <td>https://www.jpost.com/breaking-news/article-79...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class       id  num_comments  \\\n",
       "0           NaN  175au9w           682   \n",
       "1           NaN  1c7lqqt          1375   \n",
       "2           NaN  1c3pjmi          1492   \n",
       "3           NaN  175v330           456   \n",
       "4  palestisrael  1c3aiv5          5143   \n",
       "\n",
       "                                             preview            subreddit  \\\n",
       "0  https://external-preview.redd.it/35qaia1zr0YPI...        r/geopolitics   \n",
       "1  https://external-preview.redd.it/mKqHVAGaLWiUI...               r/news   \n",
       "2                                                NaN  r/NoStupidQuestions   \n",
       "3                                                NaN        r/geopolitics   \n",
       "4  https://external-preview.redd.it/8bwtWtE4wx8uq...          r/worldnews   \n",
       "\n",
       "                                                text            timestamp  \\\n",
       "0                                                NaN  2023-10-11 11:31:27   \n",
       "1                                                NaN  2024-04-19 03:26:18   \n",
       "2  Aren't they Jewish? Don't they specifically NO...  2024-04-14 10:26:07   \n",
       "3  Basically the question above. I understand the...  2023-10-12 02:44:41   \n",
       "4                                                NaN  2024-04-13 19:53:25   \n",
       "\n",
       "                                               title  upvote_ratio  upvotes  \\\n",
       "0     Is this Palestine-Israel map history accurate?          0.77     1060   \n",
       "1  Israel missiles strike Iran - US officials inf...          0.94    11643   \n",
       "2  Why do Christian nationalists support Israel, ...          0.77     6048   \n",
       "3        Why is Israel so significant for the West ?          0.87      366   \n",
       "4  Iran launched dozens of drones toward Israel -...          0.91    28884   \n",
       "\n",
       "                                                 url  \n",
       "0  https://solidarnost.su/wp-content/uploads/2015...  \n",
       "1  https://www.reuters.com/world/middle-east/isra...  \n",
       "2  https://www.reddit.com/r/NoStupidQuestions/com...  \n",
       "3  https://www.reddit.com/r/geopolitics/comments/...  \n",
       "4  https://www.jpost.com/breaking-news/article-79...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_data=pd.read_csv(\"data/posts.csv\")\n",
    "posts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10aea848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2802 entries, 0 to 2801\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   class         726 non-null    object \n",
      " 1   id            2802 non-null   object \n",
      " 2   num_comments  2802 non-null   int64  \n",
      " 3   preview       2307 non-null   object \n",
      " 4   subreddit     2802 non-null   object \n",
      " 5   text          551 non-null    object \n",
      " 6   timestamp     2802 non-null   object \n",
      " 7   title         2802 non-null   object \n",
      " 8   upvote_ratio  2802 non-null   float64\n",
      " 9   upvotes       2802 non-null   int64  \n",
      " 10  url           2802 non-null   object \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 240.9+ KB\n"
     ]
    }
   ],
   "source": [
    "posts_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c74233",
   "metadata": {},
   "source": [
    "we will train the model on two data:\n",
    "\n",
    "* Data that includes information about the post title, post text, and comments.\n",
    "* Data that includes only the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9a4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data=pd.merge(comments_data,posts_data,left_on='post_id', right_on='id', how='inner')\n",
    "sentences=[]\n",
    "for index,row in merged_data.iterrows():  \n",
    "    if pd.notnull(row[\"text_y\"]): \n",
    "        sentences.append(\"title of the post: \"+row[\"title\"]+\"\\n\"+\" post: \"+row[\"text_y\"]+\"\\n\"+\" comment: \"+row[\"text_x\"])\n",
    "    else:\n",
    "        sentences.append(\"title of the post: \"+row[\"title\"]+\"\\n\"+\" comment: \"+row[\"text_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e190bf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>len_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>with palestine</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>with palestine</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>with israel</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>with israel</td>\n",
       "      <td>4039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence           label  \\\n",
       "0  title of the post: Is this Palestine-Israel ma...         neutral   \n",
       "1  title of the post: Is this Palestine-Israel ma...  with palestine   \n",
       "2  title of the post: Is this Palestine-Israel ma...  with palestine   \n",
       "3  title of the post: Is this Palestine-Israel ma...     with israel   \n",
       "4  title of the post: Is this Palestine-Israel ma...     with israel   \n",
       "\n",
       "   len_sentence  \n",
       "0           598  \n",
       "1           263  \n",
       "2           301  \n",
       "3           823  \n",
       "4          4039  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=merged_data\n",
    "dataset[\"sentence\"]=sentences\n",
    "dataset=dataset[[\"sentence\",\"label\"]]\n",
    "dataset.loc[:, \"label\"] = dataset[\"label\"].apply(lambda x : \"neutral\" if x == \"indifferent\" or x == \"inquisitive\" else x)\n",
    "dataset.loc[:,\"len_sentence\"]=dataset[\"sentence\"].apply(lambda x: len(x.strip()))\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dfc80f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Israel pulled out of Gaza in 2005 so the 2010 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>with palestine</td>\n",
       "      <td>The 1946 map should be completely red as the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with palestine</td>\n",
       "      <td>To be fair. You should count 1947 as the first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>with israel</td>\n",
       "      <td>No. Each map is depicting something different....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with israel</td>\n",
       "      <td>A history of Israel over the last 100 years, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>with israel</td>\n",
       "      <td>Completely inaccurate. The first map removes t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I would say “misleading at best”. The maps app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>with israel</td>\n",
       "      <td>I like the part where it pretends the 1967 map...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with israel</td>\n",
       "      <td>1st is completely made up, there is no reason ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neutral</td>\n",
       "      <td>A simple no is the best answer the first map s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label                                           sentence\n",
       "0         neutral  Israel pulled out of Gaza in 2005 so the 2010 ...\n",
       "1  with palestine  The 1946 map should be completely red as the w...\n",
       "2  with palestine  To be fair. You should count 1947 as the first...\n",
       "3     with israel  No. Each map is depicting something different....\n",
       "4     with israel  A history of Israel over the last 100 years, i...\n",
       "5     with israel  Completely inaccurate. The first map removes t...\n",
       "6         neutral  I would say “misleading at best”. The maps app...\n",
       "7     with israel  I like the part where it pretends the 1967 map...\n",
       "8     with israel  1st is completely made up, there is no reason ...\n",
       "9         neutral  A simple no is the best answer the first map s..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1=merged_data[[\"label\"]]\n",
    "dataset1[\"sentence\"]=merged_data[\"text_x\"]\n",
    "dataset1.loc[:, \"label\"] = dataset1[\"label\"].apply(lambda x : \"neutral\" if x == \"indifferent\" or x == \"inquisitive\" else x)\n",
    "dataset.loc[:,\"len_sentence\"]=dataset[\"sentence\"].apply(lambda x: len(x.strip()))\n",
    "dataset1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b4b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=dataset['label'].unique()\n",
    "id2label={ key:values for key,values in enumerate(labels)}\n",
    "label2id={values:key for key,values in enumerate(labels)}\n",
    "dataset.loc[:,\"encoded_label\"]=dataset[\"label\"].map(label2id)\n",
    "dataset1.loc[:,\"encoded_label\"]=dataset1[\"label\"].map(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf4e8f",
   "metadata": {},
   "source": [
    "# the distribution of the length of sentences\n",
    "\n",
    "we have plot the distribution of each sentences in our dataset so we can decide if the input_embeddings_maxlen is suitable for training our model on the data that we have.\n",
    "\n",
    "in our case we can see that the length of the majority sentences in our dataset are between 0 and 600 so we will let the value of our input_embeddings_maxlen to its default values which is 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e998e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = range(0, max(dataset['len_sentence']) + 50, 50)\n",
    "plt.hist(dataset['len_sentence'], bins=bins, edgecolor='black', alpha=0.7)\n",
    "plt.title('Sentence Length Distribution')\n",
    "plt.xlabel('entence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c2cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "with palestine    9130\n",
      "with israel       9052\n",
      "neutral           8136\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d4a697",
   "metadata": {},
   "source": [
    "we can see that our data is well balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a0a12",
   "metadata": {},
   "source": [
    "Encoding the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d305713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'with palestine' 'with israel']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>len_sentence</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>with palestine</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>with palestine</td>\n",
       "      <td>301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>with israel</td>\n",
       "      <td>823</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>title of the post: Is this Palestine-Israel ma...</td>\n",
       "      <td>with israel</td>\n",
       "      <td>4039</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence           label  \\\n",
       "0  title of the post: Is this Palestine-Israel ma...         neutral   \n",
       "1  title of the post: Is this Palestine-Israel ma...  with palestine   \n",
       "2  title of the post: Is this Palestine-Israel ma...  with palestine   \n",
       "3  title of the post: Is this Palestine-Israel ma...     with israel   \n",
       "4  title of the post: Is this Palestine-Israel ma...     with israel   \n",
       "\n",
       "   len_sentence  encoded_label  \n",
       "0           598              0  \n",
       "1           263              1  \n",
       "2           301              1  \n",
       "3           823              2  \n",
       "4          4039              2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['label'].unique())\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6debc34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer=RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenized_text=tokenizer(dataset.sentence.to_list(), padding=\"max_length\", truncation=True, max_length=512)\n",
    "tokenized_text_comments_only=tokenizer(dataset1.sentence.to_list(), padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d07b241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_label=dataset.loc[:,\"encoded_label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe6e8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset,RandomSampler,DataLoader,SequentialSampler\n",
    "def split_data(tokenized_data):\n",
    "    \n",
    "    train_inputs, test_inputs, train_labels, test_labels=train_test_split(tokenized_text[\"input_ids\"],encoded_label, random_state=42, test_size=0.2)\n",
    "    validation_inputs,test_inputs,validation_labels,test_labels=train_test_split(test_inputs,test_labels,random_state=42, test_size=0.5)\n",
    "    train_masks, test_masks = train_test_split(tokenized_text[\"attention_mask\"],random_state=42, test_size=0.2)\n",
    "    validation_masks,test_masks=train_test_split(test_masks,random_state=42, test_size=0.5)\n",
    "\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    validation_inputs = torch.tensor(validation_inputs)\n",
    "    test_inputs=torch.tensor(test_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    validation_labels = torch.tensor(validation_labels)\n",
    "    test_labels=torch.tensor(test_labels)\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    validation_masks = torch.tensor(validation_masks)\n",
    "    test_masks=torch.tensor(test_masks)\n",
    "    \n",
    "    batch_size=32\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    return (train_dataloader,train_inputs.shape[0]),(validation_dataloader,validation_inputs.shape[0]),(test_dataloader,test_inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7df05163",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,validation,test=split_data(tokenized_text)\n",
    "train1,validation1,test1=split_data(tokenized_text_comments_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16112627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoConfig\n",
    "model=AutoModelForSequenceClassification.from_pretrained(\"roberta-base\",\n",
    "                                  num_labels=3, \n",
    "                                  problem_type=\"multi_label_classification\",\n",
    "                                  id2label=id2label,\n",
    "                                  label2id=label2id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec567fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add weight_decay to avoid overfitting\n",
    "# we add it to all the other layers except for the bias and LayerNorm\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    " {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    " 'weight_decay_rate': 0.001},\n",
    "\n",
    " {'params': [p for n, p in param_optimizer if any(nd in n for nd in\n",
    "no_decay)],\n",
    " 'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "417f7b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DataParallel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "\n",
    "def train_evaluate(model,model_name,train_data,validation_data):\n",
    "    \n",
    "    output_dir = '/kaggle/working/models/'\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    loss_set={\"train_loss\":[],\"valid_loss\":[]}\n",
    "    \n",
    "    model.to(device)\n",
    "    model = DataParallel(model)\n",
    "    \n",
    "    # we defined the number of epoch for the training\n",
    "\n",
    "    epochs = 4\n",
    "\n",
    "    training_steps=len(train_data[0]) * epochs\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=training_steps)\n",
    "    # we used the crossEntropyLoss function because we are doing multiclassification\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_model_accuracy=0\n",
    "\n",
    "    for _ in trange(epochs, desc=\"Epoch\"): \n",
    "            model.train()\n",
    "            train_loss= []\n",
    "            correct_predictions=0   \n",
    "            for step, batch in enumerate(train_data[0]):\n",
    "                    batch = tuple(t.to(device) for t in batch)\n",
    "                    input_ids, input_mask, labels = batch\n",
    "                    outputs = model(input_ids, attention_mask=input_mask)\n",
    "                    logits=outputs.logits\n",
    "                    _, preds = torch.max(logits, dim=1)\n",
    "                    loss=loss_fn(logits,labels)\n",
    "                    correct_predictions += torch.sum(preds == labels).cpu()\n",
    "                    train_loss.append(loss.item())\n",
    "                    loss.backward()\n",
    "                    # to avoid gradiant exploding\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()  \n",
    "            train_accuracy = correct_predictions / train_data[1]\n",
    "            loss_set[\"train_loss\"].append(train_loss)\n",
    "            print(f\"train_loss: {np.mean(train_loss)}\")\n",
    "            print(f\"train_accuracy : {train_accuracy}\")\n",
    "   \n",
    "            model.eval()\n",
    "            valid_losses = []\n",
    "            correct_predictions_valid = 0\n",
    "            validation_accuracy=0\n",
    "\n",
    "            for step,batch in enumerate(validation_data[0]):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                valid_input, valid_mask, labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                        outputs = model(valid_input, attention_mask=valid_mask)\n",
    "                        _,preds = torch.max(outputs.logits, dim=1)\n",
    "                        valid_loss = loss_fn(outputs.logits, labels)\n",
    "                        correct_predictions_valid += torch.sum(preds == labels).cpu()\n",
    "                        valid_losses.append(valid_loss.item())\n",
    "\n",
    "            loss_set[\"valid_loss\"].append(np.mean(valid_losses))\n",
    "            validation_accuracy = correct_predictions_valid / validation_data[1]\n",
    "            print(f\"valid_loss: {np.mean(valid_losses)}\")\n",
    "            print(f\"validation_accuracy: {validation_accuracy}\")\n",
    "            if validation_accuracy > best_model_accuracy:\n",
    "                    best_model_accuracy = validation_accuracy\n",
    "                    torch.save(model.module.state_dict(), os.path.join(output_dir, f'{model_name}.bin'))\n",
    "                    \n",
    "    return loss_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c5cf824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [05:53<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 69.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m loss_set=\u001b[43mtrain_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfine_tuned_roberta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m------------------------------fine-tuned-only-on-the-comments--------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m loss_set_comment_only=train_evaluate(model,\u001b[33m\"\u001b[39m\u001b[33mfine_tuned_roberta_comments\u001b[39m\u001b[33m\"\u001b[39m,train1,validation1)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain_evaluate\u001b[39m\u001b[34m(model, model_name, train_data, validation_data)\u001b[39m\n\u001b[32m     35\u001b[39m batch = \u001b[38;5;28mtuple\u001b[39m(t.to(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[32m     36\u001b[39m input_ids, input_mask, labels = batch\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m logits=outputs.logits\n\u001b[32m     39\u001b[39m _, preds = torch.max(logits, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:192\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     module_kwargs = ({},)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.device_ids) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[32m    194\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.parallel_apply(replicas, inputs, module_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1191\u001b[39m, in \u001b[36mRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1184\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1185\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1186\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1187\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1189\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1203\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:828\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    819\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    821\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    822\u001b[39m     input_ids=input_ids,\n\u001b[32m    823\u001b[39m     position_ids=position_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    826\u001b[39m     past_key_values_length=past_key_values_length,\n\u001b[32m    827\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    840\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    841\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:517\u001b[39m, in \u001b[36mRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    506\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    507\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    508\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         output_attentions,\n\u001b[32m    515\u001b[39m     )\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:406\u001b[39m, in \u001b[36mRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    395\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    396\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    404\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    405\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    415\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:333\u001b[39m, in \u001b[36mRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    325\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    332\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    343\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\University\\DataMining\\wiki-project\\.conda\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:253\u001b[39m, in \u001b[36mRobertaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    250\u001b[39m         relative_position_scores_key = torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mbhrd,lrd->bhlr\u001b[39m\u001b[33m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[32m    251\u001b[39m         attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m attention_scores = \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_head_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\u001b[39;00m\n\u001b[32m    256\u001b[39m     attention_scores = attention_scores + attention_mask\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.16 GiB is allocated by PyTorch, and 69.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "loss_set=train_evaluate(model,\"fine_tuned_roberta\",train,validation)\n",
    "print(\"------------------------------fine-tuned-only-on-the-comments--------------------------\")\n",
    "loss_set_comment_only=train_evaluate(model,\"fine_tuned_roberta_comments\",train1,validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def evaluate_on_testset(model_name): \n",
    "    model =  AutoModelForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                      problem_type=\"multi_label_classification\", \n",
    "                                                      num_labels=3,\n",
    "                                                      id2label=id2label,\n",
    "                                                      label2id=label2id\n",
    "                                                     )\n",
    "\n",
    "    state_dict = torch.load(f\"/kaggle/working/models/{model_name}.bin\")\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(test[0], desc=\"Evaluating on test data\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        test_input, test_mask, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "                outputs = model(test_input, attention_mask=test_mask)\n",
    "                logits = outputs.logits\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(id2label))))\n",
    "    print(cm)\n",
    "    print(classification_report(all_labels,all_preds,target_names=list(id2label.values())))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(id2label.values()), yticklabels=list(id2label.values()))\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_testset(\"fine_tuned_roberta\")\n",
    "print(\"-----------------------------evaluation of the fine_tuned_roberta_comments model------------------------------------- \")\n",
    "evaluate_on_testset(\"fine_tuned_roberta_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(title,post,commentaire,saved_model):\n",
    "    from transformers import RobertaBertTokenizer,AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    state_dict=torch.load(f\"{saved_model}\")\n",
    "    tokenizer=RobertaBertTokenizer.from_pretrained(\"roberta-base-uncased\")\n",
    "    model =  AutoModelForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                      problem_type=\"multi_label_classification\", \n",
    "                                                      num_labels=3               \n",
    "                                                     )\n",
    "    model.load_state_dict(state_dict)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    inputs = tokenizer(\"title of the post: \"+title+\"\\n\"+\"post: \"+post+\"\\n\"+\"comment: \"+commentaire, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "    id2label={\n",
    "          0:\"neutral\",\n",
    "          1:\"with palestine\",\n",
    "          2:\"with israel\"\n",
    "    }\n",
    "    print(id2label)\n",
    "    return id2label[preds.item()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
