{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2862f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import re\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3094ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK punkt already installed\n",
      "NLTK stopwords already installed\n",
      "Downloading NLTK wordnet...\n",
      "Successfully downloaded wordnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\USER\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# NLTK DATA SETUP WITH ROBUST ERROR HANDLING\n",
    "# =============================================\n",
    "def setup_nltk():\n",
    "    # Set the NLTK data path explicitly\n",
    "    nltk_data_path = os.path.join(os.path.expanduser('~'), 'nltk_data')\n",
    "    if not os.path.exists(nltk_data_path):\n",
    "        os.makedirs(nltk_data_path)\n",
    "    \n",
    "    # Set the path in NLTK\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "    \n",
    "    # List of required NLTK packages\n",
    "    required_nltk = ['punkt', 'stopwords', 'wordnet']\n",
    "    \n",
    "    for package in required_nltk:\n",
    "        try:\n",
    "            nltk.data.find(f'tokenizers/{package}' if package == 'punkt' else f'corpora/{package}')\n",
    "            print(f\"NLTK {package} already installed\")\n",
    "        except LookupError:\n",
    "            print(f\"Downloading NLTK {package}...\")\n",
    "            try:\n",
    "                nltk.download(package, download_dir=nltk_data_path)\n",
    "                print(f\"Successfully downloaded {package}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {package}: {str(e)}\")\n",
    "                # Fallback: Try downloading without specifying directory\n",
    "                nltk.download(package)\n",
    "                print(f\"Used fallback method for {package}\")\n",
    "\n",
    "# Run the setup\n",
    "setup_nltk()\n",
    "\n",
    "# Now import NLTK components\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f044780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df1 = pd.read_csv('data/reddit_comments_clean.csv')\n",
    "    df2 = pd.read_csv('data/self_collected_war_subreddit_comments.csv')\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    print(\"CSV loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {str(e)}\")\n",
    "    exit()\n",
    "\n",
    "# Verify required columns exist\n",
    "required_columns = ['text', 'label']\n",
    "if not all(col in df.columns for col in required_columns):\n",
    "    print(f\"Missing required columns. Needed: {required_columns}, Found: {df.columns.tolist()}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e730b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Basic cleaning\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenize with fallback\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except:\n",
    "            tokens = text.split()  # Simple fallback\n",
    "            \n",
    "        # Get stopwords with fallback\n",
    "        try:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "        except:\n",
    "            stop_words = set()  # Empty set if stopwords fails\n",
    "            \n",
    "        # Filter stopwords\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        # Lemmatization with fallback\n",
    "        try:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        except:\n",
    "            pass  # Skip lemmatization if fails\n",
    "            \n",
    "        return ' '.join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9e2872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "sentiment\n",
      "pro-israel       15309\n",
      "pro-palestine    11998\n",
      "neutral           8136\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Map existing labels to our target categories\n",
    "label_mapping = {\n",
    "    'with israel': 'pro-israel',\n",
    "    'with palestine': 'pro-palestine',\n",
    "    'neutral': 'neutral',\n",
    "    'inquisitive': 'neutral',  # Assuming inquisitive is neutral\n",
    "    'indifferent': 'neutral'   # Assuming indifferent is neutral\n",
    "}\n",
    "df['sentiment'] = df['label'].map(label_mapping)\n",
    "\n",
    "# Drop rows where mapping resulted in NaN (if any unexpected labels exist)\n",
    "df = df.dropna(subset=['sentiment'])\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83456cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training samples: 28354\n",
      "Test samples: 7089\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X = df['processed_text']\n",
    "y = df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154d5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating model pipeline...\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with TF-IDF and Logistic Regression\n",
    "print(\"\\nCreating model pipeline...\")\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        ngram_range=(1, 2), \n",
    "        max_features=5000,\n",
    "        min_df=5,\n",
    "        max_df=0.7\n",
    "    )),\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='multinomial', \n",
    "        solver='lbfgs', \n",
    "        max_iter=1000,\n",
    "        class_weight='balanced'  # Handle class imbalance\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05010778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ca4194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      neutral       0.50      0.57      0.53      1627\n",
      "   pro-israel       0.61      0.69      0.65      3062\n",
      "pro-palestine       0.56      0.41      0.48      2400\n",
      "\n",
      "     accuracy                           0.57      7089\n",
      "    macro avg       0.56      0.56      0.55      7089\n",
      " weighted avg       0.57      0.57      0.56      7089\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted      neutral  pro-israel  pro-palestine\n",
      "Actual                                           \n",
      "neutral            935         381            311\n",
      "pro-israel         486        2100            476\n",
      "pro-palestine      449         956            995\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating model...\")\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3bef300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...\n",
      "Model saved as 'israel_palestine_sentiment_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving model...\")\n",
    "joblib.dump(pipeline, 'israel_palestine_sentiment_model.pkl')\n",
    "print(\"Model saved as 'israel_palestine_sentiment_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d39864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model=None):\n",
    "    \"\"\"Predict sentiment of new text with optional model parameter\"\"\"\n",
    "    if model is None:\n",
    "        try:\n",
    "            model = joblib.load('israel_palestine_sentiment_model.pkl')\n",
    "        except:\n",
    "            print(\"Error loading model. Using pipeline from memory.\")\n",
    "            model = pipeline\n",
    "    \n",
    "    processed_text = preprocess_text(text)\n",
    "    if not processed_text.strip():\n",
    "        return \"neutral\"  # Default for empty text\n",
    "    \n",
    "    try:\n",
    "        return model.predict([processed_text])[0]\n",
    "    except:\n",
    "        return \"neutral\"  # Fallback prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809df313",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('israel_palestine_sentiment_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0590f1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Predictions:\n",
      "Text: 'I stand with Israel in their right to defend thems...' -> pro-israel\n",
      "Text: 'The IDF is protecting their citizens...' -> pro-palestine\n",
      "Text: 'Israel has the right to exist as a Jewish state...' -> pro-israel\n",
      "Text: 'Israel has the right to defend its borders....' -> pro-israel\n",
      "Text: 'Hamas is a terrorist organization targeting civili...' -> pro-israel\n",
      "Text: 'Supporting Israel is supporting democracy in the M...' -> pro-israel\n",
      "Text: 'The Iron Dome saves countless Israeli lives....' -> pro-israel\n",
      "Text: 'Criticism of Israel often masks antisemitism....' -> pro-palestine\n",
      "Text: 'Israeli citizens live under constant rocket threat...' -> pro-israel\n",
      "Text: 'The Jewish people have a historical right to this ...' -> pro-palestine\n",
      "Text: 'IDF operations aim to eliminate terrorist threats....' -> pro-israel\n",
      "Text: 'Israel withdrew from Gaza, yet rockets still fly....' -> pro-israel\n",
      "Text: 'The UN is biased against Israel in its resolutions...' -> pro-israel\n",
      "Text: 'Free Palestine from occupation...' -> pro-palestine\n",
      "Text: 'Palestinians deserve equal rights and freedom...' -> pro-palestine\n",
      "Text: 'End the occupation of Palestinian territories...' -> pro-palestine\n",
      "Text: 'Israel's blockade has devastated Gaza's economy....' -> pro-israel\n",
      "Text: 'The occupation must end for peace to begin....' -> pro-palestine\n",
      "Text: 'Palestinian families are being evicted from their ...' -> pro-palestine\n",
      "Text: 'The West Bank is under illegal military control....' -> pro-palestine\n",
      "Text: 'Free Gaza from siege and suffering....' -> pro-palestine\n",
      "Text: 'Palestinian children deserve safety and education....' -> pro-palestine\n",
      "Text: 'The wall separates families and stifles lives....' -> pro-palestine\n",
      "Text: 'The Nakba is an ongoing tragedy for Palestinians....' -> pro-palestine\n",
      "Text: 'Settlements violate international law....' -> pro-palestine\n",
      "Text: 'We must stand against apartheid policies....' -> pro-palestine\n",
      "Text: 'This is a neutral comment about the situation...' -> neutral\n",
      "Text: 'The conflict is complex with valid arguments on bo...' -> neutral\n",
      "Text: 'Both sides have suffered greatly in this conflict....' -> neutral\n",
      "Text: 'Dialogue and understanding are essential for peace...' -> pro-palestine\n",
      "Text: 'The conflict has a long and complex history....' -> neutral\n",
      "Text: 'Civilians on both sides deserve protection....' -> neutral\n",
      "Text: 'International law should guide the resolution proc...' -> pro-palestine\n",
      "Text: 'It's important to listen to all voices in this deb...' -> pro-palestine\n",
      "Text: 'War affects everyone, not just combatants....' -> neutral\n",
      "Text: 'Social media often simplifies complex issues....' -> neutral\n",
      "Text: 'Peace will require compromise from both parties....' -> neutral\n",
      "Text: 'We must seek truth before taking sides....' -> neutral\n",
      "\n",
      "Accuracy: 81.58%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pro_israel_texts = [\n",
    "    \"I stand with Israel in their right to defend themselves\",\n",
    "    \"The IDF is protecting their citizens\",\n",
    "    \"Israel has the right to exist as a Jewish state\",\n",
    "    \"Israel has the right to defend its borders.\",\n",
    "    \"Hamas is a terrorist organization targeting civilians.\",\n",
    "    \"Supporting Israel is supporting democracy in the Middle East.\",\n",
    "    \"The Iron Dome saves countless Israeli lives.\",\n",
    "    \"Criticism of Israel often masks antisemitism.\",\n",
    "    \"Israeli citizens live under constant rocket threat.\",\n",
    "    \"The Jewish people have a historical right to this land.\",\n",
    "    \"IDF operations aim to eliminate terrorist threats.\",\n",
    "    \"Israel withdrew from Gaza, yet rockets still fly.\",\n",
    "    \"The UN is biased against Israel in its resolutions.\"\n",
    "]\n",
    "\n",
    "# --- Pro-Palestine Texts ---\n",
    "pro_palestine_texts = [\n",
    "    \"Free Palestine from occupation\",\n",
    "    \"Palestinians deserve equal rights and freedom\",\n",
    "    \"End the occupation of Palestinian territories\",\n",
    "    \"Israel's blockade has devastated Gaza's economy.\",\n",
    "    \"The occupation must end for peace to begin.\",\n",
    "    \"Palestinian families are being evicted from their homes.\",\n",
    "    \"The West Bank is under illegal military control.\",\n",
    "    \"Free Gaza from siege and suffering.\",\n",
    "    \"Palestinian children deserve safety and education.\",\n",
    "    \"The wall separates families and stifles lives.\",\n",
    "    \"The Nakba is an ongoing tragedy for Palestinians.\",\n",
    "    \"Settlements violate international law.\",\n",
    "    \"We must stand against apartheid policies.\"\n",
    "]\n",
    "\n",
    "# --- Neutral Texts ---\n",
    "neutral_texts = [\n",
    "    \"This is a neutral comment about the situation\",\n",
    "    \"The conflict is complex with valid arguments on both sides\",\n",
    "    \"Both sides have suffered greatly in this conflict.\",\n",
    "    \"Dialogue and understanding are essential for peace.\",\n",
    "    \"The conflict has a long and complex history.\",\n",
    "    \"Civilians on both sides deserve protection.\",\n",
    "    \"International law should guide the resolution process.\",\n",
    "    \"It's important to listen to all voices in this debate.\",\n",
    "    \"War affects everyone, not just combatants.\",\n",
    "    \"Social media often simplifies complex issues.\",\n",
    "    \"Peace will require compromise from both parties.\",\n",
    "    \"We must seek truth before taking sides.\"\n",
    "]\n",
    "\n",
    "sample_texts = pro_israel_texts + pro_palestine_texts + neutral_texts\n",
    "true_labels = (\n",
    "    [\"pro-israel\"] * len(pro_israel_texts)\n",
    "    + [\"pro-palestine\"] * len(pro_palestine_texts)\n",
    "    + [\"neutral\"] * len(neutral_texts)\n",
    ")\n",
    "\n",
    "# --- Predict and evaluate ---\n",
    "predicted_labels = []\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for text in sample_texts:\n",
    "    prediction = predict_sentiment(text)\n",
    "    predicted_labels.append(prediction)\n",
    "    print(f\"Text: '{text[:50]}...' -> {prediction}\")\n",
    "\n",
    "# --- Accuracy ---\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "754b3061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: '...' -> neutral\n"
     ]
    }
   ],
   "source": [
    "test_case = \"\"\n",
    "print(f\"Text: '{test_case[:50]}...' -> {predict_sentiment(test_case)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
